/**
 * Skill Extraction
 *
 * Extracts skills from Motor Cortex workspace to data/skills/.
 *
 * After Motor Cortex creates a skill in its workspace, this module:
 * - Compares workspace files against baseline manifest
 * - Extracts only changed/added files to data/skills/
 * - Forces trust to "pending_review" for user approval
 * - Rebuilds the skills index
 *
 * ## Security
 *
 * - Symlinks are rejected throughout the skill tree
 * - Size limits enforced (1MB per file, 10MB total)
 * - Name must match frontmatter name
 * - Atomic copy prevents race conditions
 *
 * ## Baseline-Diff Extraction
 *
 * Skills are copied to workspace root on init. A baseline manifest (.motor-baseline.json)
 * records the hash of each copied file. On extraction, we:
 * - Read the baseline to know what was originally copied
 * - Scan workspace for current skill files
 * - Extract files that changed or were added
 * - Skip if no changes (prevents trust churn)
 */

import { readdir, readFile, stat, lstat, cp, rename, rm, mkdir } from 'node:fs/promises';
import { join, dirname } from 'node:path';
import { createHash } from 'node:crypto';
import type { Logger } from '../../types/logger.js';
import type { SkillInput, SkillPolicy } from '../skills/skill-types.js';
import {
  computeDirectoryHash,
  parseSkillFile,
  parseSkillInputs,
  validateSkillFrontmatter,
  savePolicy,
} from '../skills/skill-loader.js';

/**
 * Baseline manifest format.
 */
export interface MotorBaseline {
  files: Record<string, string>; // path -> sha256 hash
}

/**
 * Generate a baseline manifest for skill files in a directory.
 *
 * Scans for files matching SKILL_FILE_ALLOWLIST, excludes SKILL_FILE_DENYLIST,
 * and returns a map of relative paths to SHA-256 hashes.
 *
 * @param dir - Absolute path to scan
 * @returns Baseline manifest
 */
export function generateBaseline(dir: string): Promise<MotorBaseline> {
  return scanSkillFiles(dir).then((files) => ({ files }));
}

/**
 * Skill file allowlist for baseline generation and extraction.
 * policy.json is excluded — it's host-side only, managed by cognition layer.
 */
const SKILL_FILE_ALLOWLIST = ['SKILL.md', 'references/**', 'scripts/**'];

/**
 * Denylist of paths to never include in skill baseline or extraction.
 */
const SKILL_FILE_DENYLIST = [
  'node_modules/**',
  '.cache/**',
  '.local/**',
  '*.log',
  '.git/**',
  '.motor-baseline.json', // Baseline file itself
  '.motor-output/**', // Truncation spillover files
  'policy.json', // Host-side only — generated by extraction, not Motor
];

/**
 * Size limits for skill extraction.
 */
const MAX_FILE_SIZE = 1024 * 1024; // 1MB per file
const MAX_TOTAL_SIZE = 10 * 1024 * 1024; // 10MB total

/**
 * Scan a directory for skill files, returning relative paths and their hashes.
 *
 * @param dir - Absolute path to scan
 * @returns Map of relative paths to SHA-256 hashes
 */
async function scanSkillFiles(dir: string): Promise<Record<string, string>> {
  const files: Record<string, string> = {};

  async function scanDirectory(currentPath: string, relativePath: string): Promise<void> {
    const entries = await readdir(currentPath, { withFileTypes: true });

    for (const entry of entries) {
      const entryPath = join(currentPath, entry.name);
      const entryRelative = join(relativePath, entry.name);

      // Skip dotfiles/directories
      if (entry.name.startsWith('.')) continue;

      if (entry.isDirectory()) {
        await scanDirectory(entryPath, entryRelative);
      } else if (entry.isFile()) {
        // Check denylist patterns
        const isDenied = SKILL_FILE_DENYLIST.some((pattern) => {
          const regex = new RegExp(
            '^' + pattern.replace(/\*\*/g, '.*').replace(/\*/g, '[^/]*') + '$'
          );
          return regex.test(entryRelative);
        });
        if (isDenied) return;

        // Check allowlist patterns
        const isAllowed = SKILL_FILE_ALLOWLIST.some((pattern) => {
          const regex = new RegExp(
            '^' + pattern.replace(/\*\*/g, '.*').replace(/\*/g, '[^/]*') + '$'
          );
          return regex.test(entryRelative);
        });

        if (isAllowed) {
          const content = await readFile(entryPath);
          const hash = createHash('sha256').update(content).digest('hex');
          files[entryRelative] = hash;
        }
      }
    }
  }

  await scanDirectory(dir, '');
  return files;
}

/**
 * Recursively check for symlinks in a directory tree.
 *
 * @param dirPath - Absolute path to check
 * @throws Error if symlink found
 */
async function rejectSymlinks(dirPath: string): Promise<void> {
  const entries = await readdir(dirPath, { withFileTypes: true });

  for (const entry of entries) {
    const fullPath = join(dirPath, entry.name);

    // Use lstat to detect symlinks
    const stats = await lstat(fullPath);
    if (stats.isSymbolicLink()) {
      throw new Error(`Symlink detected: ${entry.name}`);
    }

    if (entry.isDirectory()) {
      // Skip dot directories
      if (entry.name.startsWith('.')) continue;
      // Recursively check
      await rejectSymlinks(fullPath);
    }
  }
}

/**
 * Compute total size of all files in a directory.
 *
 * @param dirPath - Absolute path to directory
 * @returns Total size in bytes
 */
async function computeTotalSize(dirPath: string): Promise<number> {
  let total = 0;
  const entries = await readdir(dirPath, { withFileTypes: true });

  for (const entry of entries) {
    const fullPath = join(dirPath, entry.name);

    if (entry.isFile()) {
      // Skip dotfiles
      if (entry.name.startsWith('.')) continue;
      const stats = await stat(fullPath);
      total += stats.size;
    } else if (entry.isDirectory()) {
      // Skip dot directories
      if (entry.name.startsWith('.')) continue;
      total += await computeTotalSize(fullPath);
    }
  }

  return total;
}

/**
 * Check if a file exceeds the size limit.
 *
 * @param dirPath - Absolute path to directory
 * @throws Error if any file exceeds MAX_FILE_SIZE
 */
async function checkFileSizeLimit(dirPath: string): Promise<void> {
  const entries = await readdir(dirPath, { withFileTypes: true });

  for (const entry of entries) {
    const fullPath = join(dirPath, entry.name);

    if (entry.isFile()) {
      // Skip dotfiles
      if (entry.name.startsWith('.')) continue;
      const stats = await stat(fullPath);
      if (stats.size > MAX_FILE_SIZE) {
        throw new Error(`File exceeds size limit (${String(MAX_FILE_SIZE)} bytes): ${entry.name}`);
      }
    } else if (entry.isDirectory()) {
      // Skip dot directories
      if (entry.name.startsWith('.')) continue;
      await checkFileSizeLimit(fullPath);
    }
  }
}

/**
 * Extract a single skill from workspace root to data/skills/.
 *
 * Uses baseline-diff to determine what changed. Only extracts if changes detected.
 *
 * @param workspace - Absolute path to workspace root (where skill files are)
 * @param baseline - Baseline manifest from workspace init
 * @param currentFiles - Current workspace files (path -> hash)
 * @param skillsDir - Absolute path to data/skills/
 * @param skillName - Name of the skill (from SKILL.md frontmatter)
 * @param runId - Run identifier for temp file naming
 * @param logger - Logger instance
 * @returns true if skill was extracted, false if skipped (no changes)
 */
async function extractSingleSkill(
  workspace: string,
  baseline: MotorBaseline,
  currentFiles: Record<string, string>,
  skillsDir: string,
  skillName: string,
  runId: string,
  logger: Logger,
  pendingCredentials?: Record<string, string>
): Promise<{ extracted: boolean; isUpdate: boolean }> {
  const skillMdPath = join(workspace, 'SKILL.md');

  // Step 1: Check SKILL.md exists
  let skillMdContent: string;
  try {
    skillMdContent = await readFile(skillMdPath, 'utf-8');
  } catch {
    logger.debug({ skillName }, 'SKILL.md missing in workspace, skipping extraction');
    return { extracted: false, isUpdate: false };
  }

  // Step 2: Parse and validate SKILL.md
  const parsed = parseSkillFile(skillMdContent);
  if ('error' in parsed) {
    logger.warn({ skillName, error: parsed.error }, 'SKILL.md parse error, skipping');
    return { extracted: false, isUpdate: false };
  }

  const validationErrors = validateSkillFrontmatter(parsed.frontmatter);
  if (validationErrors.length > 0) {
    logger.warn({ skillName, errors: validationErrors }, 'Frontmatter validation failed, skipping');
    return { extracted: false, isUpdate: false };
  }

  const nameFromFrontmatter = parsed.frontmatter['name'] as string;
  if (nameFromFrontmatter && nameFromFrontmatter !== skillName) {
    logger.warn(
      { skillName, nameFromFrontmatter },
      'Name mismatch (frontmatter vs passed name), using frontmatter name'
    );
    skillName = nameFromFrontmatter;
  }

  // Step 3: Determine what changed (three-way diff)
  const changedFiles: string[] = [];
  const deletedFiles: string[] = [];

  // CHANGED: file in baseline with different hash → extract
  // ADDED: file in current but not in baseline → extract
  // DELETED: file in baseline but not in current → remove from installed skill
  for (const [path, currentHash] of Object.entries(currentFiles)) {
    const baselineHash = baseline.files[path];
    if (baselineHash === undefined || baselineHash !== currentHash) {
      changedFiles.push(path);
    }
  }

  for (const path of Object.keys(baseline.files)) {
    if (currentFiles[path] === undefined) {
      deletedFiles.push(path);
    }
  }

  // If no changes → skip (no trust churn)
  if (changedFiles.length === 0 && deletedFiles.length === 0) {
    logger.debug({ skillName }, 'No skill changes detected, skipping extraction');
    return { extracted: false, isUpdate: false };
  }

  logger.info(
    { skillName, changed: changedFiles.length, deleted: deletedFiles.length },
    'Skill changes detected, extracting'
  );

  // Step 4: Copy ALL current skill files to temp dir (baseline-diff only detects changes,
  // the installed skill must be complete)
  const tempDir = join(skillsDir, `.tmp-${skillName}-${runId}`);
  const targetDir = join(skillsDir, skillName);
  const backupDir = join(skillsDir, `.old-${skillName}-${String(Date.now())}`);
  const isUpdate = await stat(targetDir)
    .then(() => true)
    .catch(() => false);

  try {
    // Copy only allowlisted skill files from workspace to temp
    await mkdir(tempDir, { recursive: true });
    for (const relativePath of Object.keys(currentFiles)) {
      const src = join(workspace, relativePath);
      const dest = join(tempDir, relativePath);
      await mkdir(dirname(dest), { recursive: true });
      await cp(src, dest);
    }

    // Step 5: Recursive symlink check on workspace (catch symlinks in non-allowlisted paths too)
    await rejectSymlinks(workspace);

    // Step 6: Size check on temp (only allowlisted skill files)
    await checkFileSizeLimit(tempDir);
    const totalSize = await computeTotalSize(tempDir);
    if (totalSize > MAX_TOTAL_SIZE) {
      throw new Error(`Total size exceeds limit (${String(MAX_TOTAL_SIZE)} bytes)`);
    }

    // Step 7: Read existing policy BEFORE atomic rename (to preserve credentialValues)
    // This is critical - after rename, targetDir contains new files, not old ones
    let existingCredentialValues: Record<string, string> | undefined;
    let existingPolicyForMerge: Record<string, unknown> | null = null;
    if (isUpdate) {
      try {
        const existingPolicyPath = join(targetDir, 'policy.json');
        const existing = await readFile(existingPolicyPath, 'utf-8');
        const parsed = JSON.parse(existing) as Record<string, unknown>;
        if (typeof parsed === 'object' && !Array.isArray(parsed)) {
          // Preserve credentialValues
          if (parsed['credentialValues'] && typeof parsed['credentialValues'] === 'object') {
            existingCredentialValues = parsed['credentialValues'] as Record<string, string>;
          }
          // Preserve other fields for fallback
          const { provenance, credentialValues: _credentialValues, ...restOfParsed } = parsed;
          const hasValidProvenance =
            provenance &&
            typeof provenance === 'object' &&
            'source' in provenance &&
            typeof provenance.source === 'string' &&
            'fetchedAt' in provenance &&
            typeof provenance.fetchedAt === 'string';
          existingPolicyForMerge = {
            ...restOfParsed,
            provenance: hasValidProvenance ? (provenance as SkillPolicy['provenance']) : undefined,
          };
        }
      } catch {
        // No valid existing policy
      }
    }

    // Step 8: Atomic install
    if (isUpdate) {
      await rename(targetDir, backupDir);
    }
    await rename(tempDir, targetDir);

    // Remove backup on success
    if (isUpdate) {
      await rm(backupDir, { recursive: true, force: true });
    }

    // Step 9: Build new policy
    // Note: policy.json is NOT in the workspace (excluded from container)
    // We build from existing policy (for updates) + frontmatter inputs

    // Compute content hash
    const contentHash = await computeDirectoryHash(targetDir);

    // Build provenance from existing policy only (workspace has no policy.json)
    let effectiveProvenance: SkillPolicy['provenance'] = undefined;
    if (existingPolicyForMerge) {
      const existingProvenance = existingPolicyForMerge['provenance'] as
        | Record<string, unknown>
        | undefined;
      if (
        existingProvenance &&
        typeof existingProvenance['source'] === 'string' &&
        typeof existingProvenance['fetchedAt'] === 'string'
      ) {
        effectiveProvenance = {
          source: existingProvenance['source'],
          fetchedAt: existingProvenance['fetchedAt'],
          contentHash,
        };
      }
    }

    // Get frontmatter inputs
    const frontmatterInputs = parsed.frontmatter['inputs'] as Record<string, unknown>[] | undefined;

    // Build new policy - use frontmatter inputs as primary source, fallback to existing
    const fallbackDomains = existingPolicyForMerge?.['allowedDomains'] as string[] | undefined;
    const fallbackCredentials = existingPolicyForMerge?.['requiredCredentials'] as
      | string[]
      | undefined;
    const fallbackInputs = existingPolicyForMerge?.['inputs'] as SkillInput[] | undefined;

    // Parse frontmatter inputs if present (reuse parseSkillInputs for validation + dedup)
    let inputs: SkillPolicy['inputs'] = undefined;
    if (Array.isArray(frontmatterInputs) && frontmatterInputs.length > 0) {
      const parsed = parseSkillInputs(frontmatterInputs);
      inputs = parsed.length > 0 ? parsed : undefined;
    } else if (fallbackInputs) {
      inputs = fallbackInputs;
    }

    // Merge credentialValues: existing + pending (pending takes precedence)
    const mergedCredentialValues: Record<string, string> = {
      ...(existingCredentialValues ?? {}),
      ...(pendingCredentials ?? {}),
    };

    const newPolicy: SkillPolicy = {
      schemaVersion: 1,
      trust: 'pending_review' as const,
      allowedDomains: fallbackDomains,
      requiredCredentials: fallbackCredentials,
      inputs,
      ...(effectiveProvenance && { provenance: effectiveProvenance }),
      ...(Object.keys(mergedCredentialValues).length > 0 && {
        credentialValues: mergedCredentialValues,
      }),
      extractedFrom: {
        runId,
        timestamp: new Date().toISOString(),
        changedFiles,
        deletedFiles,
      },
    };

    await savePolicy(targetDir, newPolicy);

    logger.info(
      { skillName, isUpdate, changedFiles, deletedFiles },
      'Skill extracted successfully'
    );
    return { extracted: true, isUpdate };
  } catch (error) {
    // Clean up temp dir if something went wrong (FIX: use rm with recursive force)
    try {
      await rm(tempDir, { recursive: true, force: true });
    } catch {
      // Ignore cleanup errors
    }
    throw error;
  }
}

/**
 * Extract skills from Motor Cortex workspace to data/skills/.
 *
 * Uses baseline-diff approach:
 * - Reads baseline manifest from workspace
 * - Scans workspace for current skill files
 * - Extracts only changed/added files
 * - Skips if no changes detected
 *
 * @param workspace - Absolute path to Motor Cortex workspace root
 * @param skillsDir - Absolute path to data/skills/
 * @param runId - Run identifier for temp file naming
 * @param logger - Logger instance
 * @returns Object with created/updated skill name arrays
 */
export async function extractSkillsFromWorkspace(
  workspace: string,
  skillsDir: string,
  runId: string,
  logger: Logger,
  pendingCredentials?: Record<string, string>
): Promise<{ created: string[]; updated: string[] }> {
  const created: string[] = [];
  const updated: string[] = [];

  // Read baseline manifest
  const baselinePath = join(workspace, '.motor-baseline.json');
  let baseline: MotorBaseline;

  try {
    const baselineContent = await readFile(baselinePath, 'utf-8');
    baseline = JSON.parse(baselineContent) as MotorBaseline;
  } catch {
    // No baseline = fresh workspace (new skill creation).
    // Treat all files as new additions.
    baseline = { files: {} };
  }

  // Scan current workspace files
  const currentFiles = await scanSkillFiles(workspace);

  // Check if SKILL.md exists (required for extraction)
  if (!currentFiles['SKILL.md']) {
    logger.debug({ workspace }, 'No SKILL.md in workspace, skipping extraction');
    return { created: [], updated: [] };
  }

  // Extract skill name from SKILL.md frontmatter
  const skillMdPath = join(workspace, 'SKILL.md');
  const skillMdContent = await readFile(skillMdPath, 'utf-8');
  const parsed = parseSkillFile(skillMdContent);

  if ('error' in parsed) {
    logger.warn({ error: parsed.error }, 'Failed to parse SKILL.md, skipping extraction');
    return { created: [], updated: [] };
  }

  const skillName = parsed.frontmatter['name'] as string;
  if (!skillName) {
    logger.warn('SKILL.md missing name in frontmatter, skipping extraction');
    return { created: [], updated: [] };
  }

  // Extract the skill
  try {
    const result = await extractSingleSkill(
      workspace,
      baseline,
      currentFiles,
      skillsDir,
      skillName,
      runId,
      logger,
      pendingCredentials
    );

    if (result.extracted) {
      if (result.isUpdate) {
        updated.push(skillName);
      } else {
        created.push(skillName);
      }
    }
  } catch (error) {
    logger.warn({ skillName, error }, 'Skill extraction failed, skipping');
  }

  // Note: Auto-discovery mode - no index.json to rebuild

  return { created, updated };
}
